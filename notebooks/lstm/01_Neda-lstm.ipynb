{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using MLP and LSTM cells for Pulse Shape Analysis\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The NEDA collaboration has build a multidetector to be used for Nuclear Physics.\n",
    "It is composed of scintillators sensitive to **neutrons** but also to *gamma-rays* :\n",
    "as the goal of the detector is to measure neutrons, it is then crucial to separate both type of particles !\n",
    "\n",
    "The signal of scintillation produced and collected by the photo-multiplicator is different depending of the interacting particle. Indeed, the signal is characterized by a rising time and a decay time, the decay time depending of the nature of the particle.\n",
    "\n",
    "The goal of this tutorial is to build neural networks able to efficiently tag the identified particle.\n",
    "To perform such a task, signals out of the NEDA modules are fully digitized with a sampling of 10ns over a range of 730ns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to read signals from a ROOT file "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the library able to read a ROOT file which contains a set of signals used for training\n",
    "(see https://github.com/scikit-hep/uproot if you don't have it installed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install uproot\n",
    "import uproot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More information about uproot at https://github.com/scikit-hep/uproot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, download the root file containing a set of signal consisting in entries in a ROOT TTree called training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!rm DBofSignals.root*\n",
    "! wget http://agata-ipnl.in2p3.fr/ftp/DBofSignals.root\n",
    "tree = uproot.open(\"DBofSignals.root\")['training']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get some informations on the Tree itself : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tree)\n",
    "print(tree.allkeys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there are different branches. For this tutorial, only the signals and truth are relevant.\n",
    "To get some entries of the tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.arrays()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen 75 bin are used for each signal. Indeed, the last two bins are used to store different informations : \n",
    "* bin 73 contains the integral of the signal since the real signal is normalized before being ranged from bin 0 to 73\n",
    "\n",
    "* bin 74 contains a TDC value i.e. the arrival time of the signal with respect to a reference (pulsed beam)\n",
    "\n",
    "\n",
    "For each entry, the branch truth contains wether or not the signal correponds to a gamma-ray 0 or a neutron 1\n",
    "\n",
    "Now plot the distribution of TDC, Integral and some signals corresponding to gamma-ray and neutrons\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the truth distribution to see the proportion of gammas and neutrons is equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_t = tree.array('truth')\n",
    "plt.hist(x_t,color='green', bins=10)\n",
    "plt.title('The gamma-neutron distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the distribution of the amplitude of all the signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_s = tree.array('signals')\n",
    "x_a = x_s[:,73]\n",
    "plt.hist(x_a,color='blue', bins=1000)\n",
    "plt.title('The Amplitude Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the distribution of the TDC of all the signals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot and compare normalized signals for gamma and neutron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_i = tree.array('indices')\n",
    "x_s = tree.array('signals')\n",
    "n = 73\n",
    "plt.figure(figsize=(30,10))\n",
    "print('First signal of the DB is a gamma-ray',x_t[0])\n",
    "plt.scatter(x_i[0,:n],x_s[0,:n], s=100, color ='red', alpha=0.5, label='Signal for a gamma-ray')\n",
    "print('Second signal of the DB is a neutron',x_t[1])\n",
    "plt.scatter(x_i[1,:n],x_s[1,:n], s=100, color ='blue', alpha=0.5, label='Signal for a neutron')\n",
    "plt.xlabel('Time (10ns/bin)')\n",
    "plt.ylabel('Normalized Amplitude')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the same plot but showing real signals i.e. not normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,10))\n",
    "x_tsg = x_s[0,73]*x_s[0]\n",
    "x_tsn = x_s[1,73]*x_s[1]\n",
    "plt.scatter(x_i[0,:n],x_tsg[:n], s=100, color ='red', alpha=0.7, label='Signal for a gamma-ray')\n",
    "plt.scatter(x_i[1,:n],x_tsn[:n], s=100, color ='blue', alpha=0.5, label='Signal for a neutron')\n",
    "plt.xlabel('Time (10ns/bin)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tdc = x_s[:,74]\n",
    "plt.hist(x_tdc,color='magenta', bins=100)\n",
    "plt.title('The TDC Distribution')\n",
    "plt.xlim(0,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to declare a Neural Network\n",
    "\n",
    "Import the TensorFlow, Keras and numpy libraries :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a model for a fully connected network (dense layers) with 4 layers :\n",
    "* 75 neurons on the first layer (input)\n",
    "* 10 neurons\n",
    "* 4 neurons\n",
    "* 2 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = keras.Sequential()\n",
    "mlp.add(keras.layers.Dense(10, activation='relu', input_shape=(75,)))\n",
    "mlp.add(keras.layers.Dense(4, activation='relu'))\n",
    "mlp.add(keras.layers.Dense(2, activation='softmax', name='output_layer'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use ReLU activation functions for the first layers and a softmax activation function for the last layer to unsure that the sum of the 2 neuron's values is 1.\n",
    "We can ask for a description of our network :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us informations about the 3 layers we have created : 3 dense layers with 10, 4 and 2 neurons. The number of trainable parameters is given as well :\n",
    "* 760 for the first layer (75 * 10 weights + 10 bias)\n",
    "* 44 for the second layer (10 * 4 weights + 4 bias)\n",
    "* 10 for the third layer (4 * 2 weights + 2 bias)\n",
    "\n",
    "So a total of __814 parameters__ for our little model! \n",
    "\n",
    "Our Multi Layer Perceptron is complete, it is composed of one input layer of 75 neurons, 2 hidden layers of respectively 10 and 4 neurons and an output layer of 2 neurons. The idea is that the first output neuron will stand for *this is a gamma signal* and the second output neuron will stand for *this is a neutron signal*. Idealy, a gamma signal would output [1,0] and a neutron signal would output [0,1].\n",
    "\n",
    "We could use our network right away, but as the weights and bias are currently random values chances are it will not be very effective. Statisticaly, it should give around 50% of good answers : as well as flipping a coin... To improve that, let's train our network!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to train the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is to feed to the network a signal along with the correct output. We will do so by batches of 100 signals+answers. At each step we will compute the error, ie the distance between the given answer and the provided truth. We will then slightly change the network parameters in order to reduce this distance and start again.\n",
    "\n",
    "We need to provide informations for the training process :\n",
    "* Which loss function to use (how do we compute the error)\n",
    "* Which optimizer to use (how do we perform the back propagation)\n",
    "* Which metrics should be computed at each step of the process\n",
    "\n",
    "For example :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.SGD(), metrics=[keras.metrics.categorical_accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can come back later to this line and tests other [loss functions](https://keras.io/losses/) and [optimizers](https://keras.io/optimizers/) and change their parameters.\n",
    "\n",
    "We now need to format our data to be able to give input values and correct answers to the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth = tree.array(\"truth\")\n",
    "signals = tree.array(\"signals\")\n",
    "\n",
    "#Number of events we want to use\n",
    "nb_events = len(truth)\n",
    "\n",
    "#We separate data in 2 sets : 1 for training, the other one for final validation\n",
    "input_data = signals[:nb_events-20000]\n",
    "input_truth = tf.keras.utils.to_categorical(truth[:nb_events-20000])\n",
    "\n",
    "validation_data = signals[nb_events-20000:nb_events]\n",
    "validation_truth = tf.keras.utils.to_categorical(truth[nb_events-20000:nb_events])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have our data: let's go! \n",
    "We give the input values and correct answers to the network plus :\n",
    "* epochs : the number of times we want to run on all our data\n",
    "* batch_size : how many values are used at once\n",
    "* validation_split : percentage of data used for validation (and not for the training). After each epoch, the network will be tested on these data.\n",
    "\n",
    "We will also use *callbacks* :\n",
    "* EarlyStopping : Used to stop the training if the loss is not improving on the validation dataset for 10 epochs, even if we did not reach the set number of epochs (smells like overtraining...).\n",
    "* Tensorboard : Used to log training info (see later)\n",
    "* ModelCheckpoint : Used to save our model in a file each time we set a record on the validation dataset (keep our champion!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.fit(input_data, input_truth, epochs=200, batch_size=100, validation_split=0.1, callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='min'),keras.callbacks.TensorBoard(\"logs/mlp_neda\"),keras.callbacks.ModelCheckpoint('mlp_neda.hdf5', monitor='val_loss', save_best_only=True, mode='min')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to test the training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our network is trained, we can test it on the 20 000 last signals (not used during training!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp=load_model(\"mlp_neda.hdf5\") # This is our champion\n",
    "score = mlp.evaluate(validation_data, validation_truth, batch_size=20000)\n",
    "print(\"Proportion of correct predictions : \",score[1]*100,\"%\")\n",
    "print(\"Loss : \",score[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing with some parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now play with the different parameters to check how it affects the training time and the final accuracy of the network. Some ideas :\n",
    "* you can use TensorBoard to have a look at the plots created during training and the graph of operations executed by the network : you should have a *TensorBoard* link at the bottom right corner of this page.\n",
    "* number of different signals used\n",
    "* size of the batch\n",
    "* number of epochs\n",
    "* activation functions used (relu?, sigmo√Ød?)\n",
    "* loss function, gradient descent optimizer, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a recursive network (Long Short Term Memory)\n",
    "\n",
    "We will now create a second neural network to perform the same task, but this time using LSTM cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the network :\n",
    "* We start by rounding the input values : this is not mandatory but might speed up the training\n",
    "* LSTM layers need a 3D input vector : [nb_signals, nb_timesteps, nb_features] but our input is [nb_signals, nb_bins]. We reshape our vector (nb_signals, 75) in (nb_signals, 75, 1) : 75 timesteps of 1 value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = 50\n",
    "\n",
    "lstm = keras.Sequential()\n",
    "lstm.add(keras.layers.Lambda(lambda x : keras.backend.round(x*100), input_shape=(75,)))\n",
    "lstm.add(keras.layers.Reshape((75,1)))\n",
    "lstm.add(keras.layers.LSTM(state_size))\n",
    "lstm.add(keras.layers.Dense(2, activation='softmax', name='output_layer'))\n",
    "lstm.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[keras.metrics.categorical_accuracy])\n",
    "lstm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training :\n",
    "\n",
    "We now have 10 502 parameters and a more complex network, epochs will take more time: start with small values!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm.fit(input_data, input_truth, epochs=15, batch_size=200, validation_split=0.1, callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, verbose=1, mode='min'),keras.callbacks.TensorBoard(\"logs/lstm_neda\"),keras.callbacks.ModelCheckpoint('lstm_neda.hdf5', monitor='val_loss', save_best_only=True, mode='min')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the trained network :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = load_model(\"lstm_neda.hdf5\") # This is our champion\n",
    "score = lstm.evaluate(validation_data, validation_truth, batch_size=20000)\n",
    "print(\"Proportion of correct predictions : \",score[1]*100,\"%\")\n",
    "print(\"Loss : \",score[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! I have roughly the same results with much more computing time... Why on hearth would I want that??\n",
    "\n",
    "Let's play a bit with the signals: there are many reasons that could cause our signals to be not so perfectly synchronyzed : bad calibration, changing the length of a fiber, ... We will simulate these real life conditions by inserting some zero values at the begining of the signals.\n",
    "\n",
    "### Creation of data sets with a shift of 1,2 and 3 bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(validation_data.shape)\n",
    "#Take the 72 first signals + 2 last ones\n",
    "shift_1 = np.append(validation_data[:,:72],validation_data[:,73:75],axis=1)\n",
    "#Add a zero at the begining\n",
    "shift_1 = np.insert(shift_1,[0],0,axis=1)\n",
    "print(shift_1.shape)\n",
    "#Take the 71 first signals + 2 last ones\n",
    "shift_2 = np.append(validation_data[:,:71],validation_data[:,73:75],axis=1)\n",
    "#Add 2 zeros at the begining\n",
    "shift_2 = np.insert(shift_2,[0,0],0,axis=1)\n",
    "print(shift_2.shape)\n",
    "#Take the 70 first signals + 2 last ones\n",
    "shift_3 = np.append(validation_data[:,:70],validation_data[:,73:75],axis=1)\n",
    "#Add 3 zeros at the begining\n",
    "shift_3 = np.insert(shift_3,[0,0,0],0,axis=1)\n",
    "print(shift_3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying the signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 75\n",
    "nb_sig = 2000\n",
    "indices = np.reshape(x_i.flatten(),(x_i.shape[0],75))\n",
    "x_data = indices[:nb_sig,:n]\n",
    "x_data=np.reshape(x_data,(x_data.shape[0]*n))\n",
    "\n",
    "print(\"No shift\")\n",
    "y_data = validation_data[:nb_sig,:n]\n",
    "y_data=np.reshape(y_data,(y_data.shape[0]*n))\n",
    "\n",
    "plt.figure(figsize=(30,10))\n",
    "plt.hist2d(x_data,y_data,(n,100))\n",
    "plt.xlabel('Time (10ns/bin)')\n",
    "plt.ylabel('Normalized Amplitude')\n",
    "plt.show()\n",
    "\n",
    "print(\"+1 bin shift\")\n",
    "y_data = shift_1[:nb_sig,:n]\n",
    "y_data=np.reshape(y_data,(y_data.shape[0]*n))\n",
    "\n",
    "plt.figure(figsize=(30,10))\n",
    "plt.hist2d(x_data,y_data,(n,100))\n",
    "plt.xlabel('Time (10ns/bin)')\n",
    "plt.ylabel('Normalized Amplitude')\n",
    "plt.show()\n",
    "\n",
    "print(\"+2 bins shift\")\n",
    "y_data = shift_2[:nb_sig,:n]\n",
    "y_data=np.reshape(y_data,(y_data.shape[0]*n))\n",
    "\n",
    "plt.figure(figsize=(30,10))\n",
    "plt.hist2d(x_data,y_data,(n,100))\n",
    "plt.xlabel('Time (10ns/bin)')\n",
    "plt.ylabel('Normalized Amplitude')\n",
    "plt.show()\n",
    "\n",
    "print(\"+3 bins shift\")\n",
    "y_data = shift_3[:nb_sig,:n]\n",
    "y_data=np.reshape(y_data,(y_data.shape[0]*n))\n",
    "\n",
    "plt.figure(figsize=(30,10))\n",
    "plt.hist2d(x_data,y_data,(n,100))\n",
    "plt.xlabel('Time (10ns/bin)')\n",
    "plt.ylabel('Normalized Amplitude')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the networks on the different data sets\n",
    "\n",
    "We now have 4 data-sets : the first one is very well synchronized with the training set, the others are respectively shifted by 1,2 and 3 bins. We'll check the evolution of our networks accuracy on the different data-sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_mlp=np.zeros(4)\n",
    "score_mlp[0] = mlp.evaluate(validation_data, validation_truth, batch_size=20000)[1]\n",
    "score_mlp[1] = mlp.evaluate(shift_1, validation_truth, batch_size=20000)[1]\n",
    "score_mlp[2] = mlp.evaluate(shift_2, validation_truth, batch_size=20000)[1]\n",
    "score_mlp[3] = mlp.evaluate(shift_3, validation_truth, batch_size=20000)[1]\n",
    "\n",
    "score_lstm=np.zeros(4)\n",
    "score_lstm[0] = lstm.evaluate(validation_data, validation_truth, batch_size=20000)[1]\n",
    "score_lstm[1] = lstm.evaluate(shift_1, validation_truth, batch_size=20000)[1]\n",
    "score_lstm[2] = lstm.evaluate(shift_2, validation_truth, batch_size=20000)[1]\n",
    "score_lstm[3] = lstm.evaluate(shift_3, validation_truth, batch_size=20000)[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(score_mlp)\n",
    "print(score_lstm)\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(score_mlp,label=\"MLP\")\n",
    "plt.plot(score_lstm,label=\"LSTM\")\n",
    "plt.xlabel('Shift (bins)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title(\"Evolution of accuracy with signal shift\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the stochastic nature of the training results may vary, but you should be able to see a better robustness of the LSTM network to time shifts in the signals."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
