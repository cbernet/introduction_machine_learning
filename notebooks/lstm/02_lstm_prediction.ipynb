{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using LSTM to detect anomalies in time series data\n",
    "\n",
    "We will train a neural network based on LSTM cells to predict future values in time series data (temperature and pressures on a cryostat). The network should be pretty good in its predictions while the context is conform with the training set, much less so when unexpected behaviours occure. By monitoring the errors done in the predictions, we can deduce that we are in a \"normal\" context or an \"abnormal\" one.\n",
    "\n",
    "## Importing modules\n",
    "\n",
    "We will use [pandas](https://pandas.pydata.org/) to load the CSV data files, [numpy](https://numpy.org/) to deal with the data arrays and [matplotlib](https://matplotlib.org) for plotting data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[SciKit-learn](https://scikit-learn.org/stable/) will help us re-scaling the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.preprocessing as preproc\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow and Keras for Machine Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "\n",
    "The training and test data are available in CSV files. First we download the files :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm *.csv\n",
    "!wget ftp://lyoftp.in2p3.fr/baulieu/nn_data/2020-02-23.csv\n",
    "!wget ftp://lyoftp.in2p3.fr/baulieu/nn_data/event.csv\n",
    "!wget ftp://lyoftp.in2p3.fr/baulieu/nn_data/event2.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas has a function to load a CSV file into a [DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) object :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"2020-02-23.csv\")\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each minute we have 4 values :\n",
    "  * cernox_4k : temperature in Kelvin\n",
    "  * K3 : pressure in millibar\n",
    "  * K4 : pressure in millibar\n",
    "  * K5 : pressure in millibar\n",
    "  \n",
    "The pressures are measured at different points on the injection system.\n",
    "  \n",
    "We don't need the timestamps, we can drop the column :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=['Date (UTC)'])\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the data! We can plot the different columns using the index number in abcisse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.plot(use_index=True,y=\"cernox_4k\",figsize=(20,3))\n",
    "data.plot(use_index=True,y=\"K3\",figsize=(20,3))\n",
    "data.plot(use_index=True,y=\"K4\",figsize=(20,3))\n",
    "data.plot(use_index=True,y=\"K5\",figsize=(20,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see our data are not in the same ranges : values for cernox_4k are in [4.04,4.09], while K5 is in [910,945]. This is a problem for the training as the goal is to minimize errors : the errors on K5 will overshadow those on cernox_4k. \n",
    "\n",
    "We need to rescale our data in the same range [0,1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rescaling the data\n",
    "\n",
    "First, we can extract the values as Numpy arrays :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = data.values\n",
    "columns = data.columns[:] #Names of the columns\n",
    "print(columns)\n",
    "print(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's rescale our data! We will use the [MinMaxScaler](\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler\") object of the scikit module to bring all informations in the [0,1] range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preproc.MinMaxScaler(feature_range=(0, 1))\n",
    "scaled = scaler.fit_transform(values)\n",
    "print(scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the modified data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot each column\n",
    "plt.figure(figsize=(20,10))\n",
    "for group in range(len(columns)):\n",
    "    plt.subplot(len(columns), 1, group+1)\n",
    "    plt.plot(scaled[:, group])\n",
    "    print('STDEV of ',columns[group],' : ',np.std(scaled[:, group]))\n",
    "    print('MEAN of ',columns[group],' : ',np.mean(scaled[:, group]))\n",
    "    plt.title(columns[group], y=0.8, loc='right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formating the training data\n",
    "\n",
    "The idea of the neural network is to guess the values of the 16th minute from the values of the 15 previous minutes. Currently, the fomat of our data is:\n",
    "\n",
    "```\n",
    "cernox_4k(T0) K3(T0) K4(T0) K5(T0)\n",
    "cernox_4k(T1) K3(T1) K4(T1) K5(T1)\n",
    "cernox_4k(T2) K3(T2) K4(T2) K5(T2)\n",
    "cernox_4k(T3) K3(T3) K4(T3) K5(T3)\n",
    "...\n",
    "```\n",
    "\n",
    "First, we want to have 16 minutes of data on one line :\n",
    "```\n",
    "cernox_4k(T0) K3(T0) K4(T0) K5(T0) cernox_4k(T1) K3(T1) K4(T1) K5(T1) ... cernox_4k(T15) K3(T15) K4(T15) K5(T15)\n",
    "cernox_4k(T1) K3(T1) K4(T1) K5(T1) cernox_4k(T2) K3(T2) K4(T2) K5(T2) ... cernox_4k(T16) K3(T16) K4(T16) K5(T16)\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_features = 4 # cernox_4k, K3, K4, K5\n",
    "nb_timesteps = 15 # number of minutes in input\n",
    "# Allocation of the array for training data\n",
    "training_data = np.zeros((len(scaled)-nb_timesteps,(nb_timesteps+1)*nb_features))\n",
    "# Take 16 consecutive lines and reshape them in one, increment starting point by one minute, restart\n",
    "for i in range(len(scaled)-nb_timesteps):\n",
    "    training_data[i] = np.reshape(scaled[i:i+nb_timesteps+1],((nb_timesteps+1)*nb_features))\n",
    "print(training_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each line is now composed of 16 minutes of consecutive data. We will separate them in a training set and a testing set. To avoid any bias, we will first shuffle all lines. We will also separate input from output : 15x4 first columns will be input, 1x4 last columns will be output (guess the 16th minute from the previous 15 minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shuffle all lines\n",
    "rng = np.random.default_rng()\n",
    "rng.shuffle(training_data)\n",
    "# split into input and outputs (15 minutes of data in input, the 16th minute in output)\n",
    "# keeping last 200 lines as test data\n",
    "train_x, train_y = training_data[:-200, :-nb_features], training_data[:-200, -nb_features:]\n",
    "test_x, test_y = training_data[-200:, :-nb_features], training_data[-200:, -nb_features:]\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An LSTM layer needs a 3D input : \n",
    "\n",
    "  * Nb records\n",
    "  * Nb timesteps\n",
    "  * Nb features\n",
    "\n",
    "So we need to reshape the inputs to give the number of timesteps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape input to be 3D [samples, timesteps, features] -> [nb_samples, nb_timesteps minutes, nb_features features]\n",
    "train_x = train_x.reshape((train_x.shape[0], nb_timesteps, nb_features))\n",
    "test_x = test_x.reshape((test_x.shape[0], nb_timesteps, nb_features))\n",
    "print(train_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In input, we have N records composed of 15 timesteps of 4 values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and training the neural network\n",
    "\n",
    "We will keep it simple with only 3 layers: \n",
    "  * Rounding the values (we don't need that precision)\n",
    "  * LSTM layer\n",
    "  * Dense layer\n",
    "\n",
    "Of course, you can test different values for the different parameters..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = keras.Sequential()\n",
    "lstm.add(keras.layers.Lambda(lambda x : keras.backend.round(x*100), input_shape=(train_x.shape[1], train_x.shape[2])))\n",
    "lstm.add(keras.layers.LSTM(400))\n",
    "lstm.add(keras.layers.Dense(nb_features, activation='sigmoid'))\n",
    "lstm.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "lstm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train the network with our traning set. We use a callback to stop the training if the val_loss shows no progress.\n",
    "\n",
    "For each entry (15 minutes of data), we try to create the data of the 16th minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit network\n",
    "history = lstm.fit(train_x, train_y, epochs=500, batch_size=240, validation_split=0.1, callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, verbose=1, mode='min'),keras.callbacks.TensorBoard(\"logs/lstm_pred\"),keras.callbacks.ModelCheckpoint('lstm.hdf5', monitor='val_loss', save_best_only=True, mode='min')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the evolution of the loss on the training and validation data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'][:], label='training')\n",
    "plt.plot(history.history['val_loss'][:], label='validation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now our trained network, are we able to guess the correct values for our test data-set? We give 15 minutes of data in input and compare the output with the 16th minute data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = load_model(\"lstm.hdf5\")\n",
    "evaluation = lstm.evaluate(test_x,test_y)\n",
    "print(\"Average error on test data :\",round(evaluation,6))\n",
    "print(\"->\",round(math.sqrt(evaluation)*100,1),\"% error on average\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the network to detect anomalies\n",
    "\n",
    "Our network is able to guess the correct future values with an accuracy of ~3% in the context of the training data. What happens if we change that context? We will check the evolution of the error using the data from the event.csv file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "strange_event = pd.read_csv(\"event.csv\")\n",
    "#Drop the timestamp column\n",
    "strange_event = strange_event.drop(columns=['Date (UTC)'])\n",
    "strange_event = strange_event.values\n",
    "#Rescale the data, just like the training set\n",
    "strange_event_scaled = scaler.transform(strange_event)\n",
    "#Plotting the data\n",
    "plt.plot(strange_event_scaled[:,0],label=\"cernox_4k\")\n",
    "plt.plot(strange_event_scaled[:,1],label=\"K3\")\n",
    "plt.plot(strange_event_scaled[:,2],label=\"K4\")\n",
    "plt.plot(strange_event_scaled[:,3],label=\"K5\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, all values are suddently increasing after around 140 minutes, which is a behaviour that is not present in the training data. We will format the data as we did for the training set :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_data = np.zeros((len(strange_event_scaled)-nb_timesteps,(nb_timesteps+1)*nb_features))\n",
    "for i in range(len(strange_event_scaled)-nb_timesteps):\n",
    "    event_data[i] = np.reshape(strange_event_scaled[i:i+nb_timesteps+1],((nb_timesteps+1)*nb_features))\n",
    "# split into input and outputs (15 minutes of data in input, the 16th minute in output)\n",
    "event_x, event_y = event_data[:, :-nb_features], event_data[:, -nb_features:]\n",
    "# reshape input to be 3D [samples, timesteps, features] -> [nb_samples, nb_timesteps minutes, nb_features features]\n",
    "event_x = event_x.reshape((event_x.shape[0], nb_timesteps, nb_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the neural network\n",
    "\n",
    "We will now send in the network the data from T0 to T14 and try to guess T15. We keep the error done in the prediction and restart with data from T1 to T15, then T2 to T16 and so on..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors=np.zeros(len(event_x))\n",
    "for i in range(len(event_x)):\n",
    "    errors[i]=lstm.evaluate(event_x[i:i+1,:,:],event_y[i:i+1],verbose=0)\n",
    "    \n",
    "fig = plt.figure(figsize=(15,9))\n",
    "plt.title(\"Evolution of the prediction's errors\")\n",
    "plt.plot(errors)\n",
    "plt.show()\n",
    "\n",
    "#Apply a sliding average on the output\n",
    "N = 10\n",
    "errors = np.convolve(errors, np.ones((N,))/N, mode='valid')\n",
    "    \n",
    "fig = plt.figure(figsize=(15,9))\n",
    "plt.title(\"Evolution of the prediction's errors (average on \"+str(N)+\" minutes)\")\n",
    "plt.plot(errors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the prediction error is relatively stable except for a huge spike: it should be easy to set a threshold to raise an alarm!\n",
    "\n",
    "Now this example is fairly obvious, you can try again with the data from file event2.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
